---
layout:     post
title:      "利用python正则表达式爬取百度新闻"
subtitle:   "python-re-docx"
date:        2019-09-15 08:55:13
author:     "RayChou"
header-img: "img/post_bg_python.jpeg"
tags:
   - 爬虫 
---
# 利用python正则表达式爬取百度新闻
利用python正则表达式，我从百度新闻、搜狗新闻、新浪财经爬取新闻舆情数据，对于大部分上市公司来说，通过以下方式足以实现新闻舆情监控与预警
* 第一步：获取网页源代码；
* 第二步：通过正则表达式提取想要的内容；
* 第三步：数据清洗；
* 第四步：定义函数并通过循环调用函数进行批量爬取

### 知识点
requests headers和timeout
file的创建、写入、关闭
异常捕获
### 爬取百度
* 24小时循环爬取
```
#-*-coding:utf-8-*-
#Author:raychou
'''
实战-百度新闻网页源代码获取
'''
import requests
import re
import time

#浏览器的header可以通过在url栏输入about:version，即可查看当前浏览器的用户代理
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safa-ri/537.36'}

def baidu(company):
    #rtt=1,按照焦点排序，百度默认按照此排序，rtt=4 按照时间排序
    url = "https://www.baidu.com/s?rtt=4&bsst=1&cl=2&tn=news&word="+company
    res = requests.get(url,headers=headers).text

    p_info = '<p class="c-author">(.*?)</p>'
    p_href = '<h3 class="c-title">.*?<a href="(.*?)"'
    p_title = '<h3 class="c-title">.*?>(.*?)</a>'
    info = re.findall(p_info, res, re.S)
    href = re.findall(p_href,res,re.S)
    title = re.findall(p_title,res,re.S)

    source = []  # 先创建两个空列表, 分别用于存储分割后的来源和日期
    date = []
    for i in range(len(info)):
        title[i] = title[i].strip()
        title[i] = re.sub('<.*?>','',title[i])

        info[i] = re.sub('<.*?>', '', info[i])
        source.append(info[i].split('&nbsp;&nbsp;')[0])
        date.append(info[i].split('&nbsp;&nbsp;')[1])
        source[i] = source[i].strip()
        date[i] = date[i].strip()

    filebaidu = open("D:\\python\\百度新闻数据挖掘报告.txt",'a')
    filebaidu.write(company + ' 数据挖掘completed!' + '\n' + '\n')
    for i in range(len(info)):
        filebaidu.write(str(i) + '.' + title[i] + '(' + date[i] + '-' + source[i] + ')'+'\n')
        filebaidu.write(href[i] + '\n')
    filebaidu.write('-----------------' + '\n' + '\n')
    filebaidu.close()

companys = [' 华能信托',' 阿里巴巴',' 万科集团',' 百度集团',' 腾讯',' 京东']
'''
24小时不间断爬取，如果数据存储到数据库后，可以考虑去重
'''
while True:
    for i in companys:
        try:
            baidu(i)
            print(i + '百度新闻爬取成功')
        except:
            print(i + '百度新闻爬取失败')
    time.sleep(10800)


```
* 爬取多页数据
```
#-*-coding:utf-8-*-
#Author:raychou
import requests
import re
#浏览器的header可以通过在url栏输入about:version，即可查看当前浏览器的用户代理
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safa-ri/537.36'}


def baidu(company,page):
    num = (page -1) * 10
    url = "https://www.baidu.com/s?rtt=4&bsst=1&cl=2&tn=news&word=" + company + "&pn=" + str(num)
    res = requests.get(url,headers=headers,timeout=100).text

    p_info = '<p class="c-author">(.*?)</p>'
    p_href = '<h3 class="c-title">.*?<a href="(.*?)"'
    p_title = '<h3 class="c-title">.*?>(.*?)</a>'
    info = re.findall(p_info, res, re.S)
    href = re.findall(p_href,res,re.S)
    title = re.findall(p_title,res,re.S)

    source = []  # 先创建两个空列表, 分别用于存储分割后的来源和日期
    date = []
    for i in range(len(info)):
        title[i] = title[i].strip()
        title[i] = re.sub('<.*?>','',title[i])

        info[i] = re.sub('<.*?>', '', info[i])
        source.append(info[i].split('&nbsp;&nbsp;')[0])
        date.append(info[i].split('&nbsp;&nbsp;')[1])
        source[i] = source[i].strip()
        date[i] = date[i].strip()

    filebaidu = open("D:\\python\\百度新闻数据挖掘报告.txt",'a')
    filebaidu.write(company + '第' + str(page+1) + ' 页数据挖掘completed!' + '\n' + '\n')
    for i in range(len(info)):
        filebaidu.write(str(i) + '.' + title[i] + '(' + date[i] + '-' + source[i] + ')'+'\n')
        filebaidu.write(href[i] + '\n')
    filebaidu.write('-----------------' + '\n' + '\n')
    filebaidu.close()

companys = [' 华能信托',' 阿里巴巴',' 万科集团',' 百度集团',' 腾讯',' 京东']

for company in companys:
    for i in range(20):
        try:
            baidu(company,i)
            print(company + '第' + str(i+1) + '页百度新闻爬取成功')
        except:
            print(company + '第' + str(i+1) + '页百度新闻爬取失败')

```
###### 执行结果
![](/img/20190915/baidu.png)

### 爬取搜狗
```
#-*-coding:utf-8-*-
#Author:raychou
import requests
import re
#浏览器的header可以通过在url栏输入about:version，即可查看当前浏览器的用户代理
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safa-ri/537.36'}

def sogou(company):
    url = "https://news.sogou.com/news?mode=1&sort=0&fixrank=1&query=" + company + "&shid=djt1"
    res = requests.get(url=url,headers=headers).text
    p_href = '<a href="(.*?)" id="uigs_.*?" target="_blank">'
    href = re.findall(p_href,res)
    p_title = '<a href=".*?" id="uigs_.*?" target="_blank">(.*?)</a>'
    title = re.findall(p_title,res)
    p_date = '<p class="news-from">.*?&nbsp;(.*?)</p>'
    date = re.findall(p_date,res)

    filesougou = open("D:\\python\\搜狗新闻数据挖掘报告.txt",'a')
    filesougou.write('\n'+company +' 数据挖掘completed!' + '\n' + '\n')
    for i in range(len(title)):
        title[i] = re.sub('<.*?>','',title[i])
        date[i] = re.sub('<.*?>','',date[i])
        filesougou.write(str(i+1)+'.'+ title[i] + '-' + date[i]+'\n')
        filesougou.write(href[i]+'\n')
    filesougou.close()

companys = [' 华能信托',' 阿里巴巴',' 万科集团',' 百度集团',' 腾讯',' 京东']

for company in companys:
    try:
        sogou(company)
        print(company + '搜狗新闻爬取成功')
    except:
        print(company + '搜狗新闻爬取失败')
```
###### 执行结果
![](/img/20190915/sogou.png)

### 爬取新浪财经
```
#-*-coding:utf-8-*-
#Author:raychou
import  requests
import re
#浏览器的header可以通过在url栏输入about:version，即可查看当前浏览器的用户代理
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safa-ri/537.36'}

def sina(company):
    url = "https://search.sina.com.cn/?q=" + company + "&range=all&c=news&sort=time&ie=utf-8"
    res = requests.get(url=url,headers=headers).text
    p_href = '<h2><a href="(.*?)" target="_blank">'
    href = re.findall(p_href,res)
    p_title = '<h2><a href=".*?" target="_blank">(.*?)</a>'
    title = re.findall(p_title,res)
    p_date = '<span class="fgray_time">.*? (.*?)</span>'
    date = re.findall(p_date,res)

    filesina = open("D:\\python\\新浪财经数据挖掘报告.txt",'a')
    filesina.write('\n'+company +' 数据挖掘completed!' + '\n' + '\n')
    for i in range(len(href)):
        filesina.write(str(i+1)+'.'+ title[i] + '-' + date[i]+'\n')
        filesina.write(href[i]+'\n')
    filesina.close()

companys = [' 华能信托',' 阿里巴巴',' 万科集团',' 百度集团',' 腾讯',' 京东']

for company in companys:
    try:
        sina(company)
        print(company + '新浪财经新闻爬取成功')
    except Exception as e:
        print(company + '新浪财经新闻爬取失败')
        print(str(e))
```

###### 执行结果
![](/img/20190915/sina.jpg)

#### 如果网站内容是动态渲染出来的，如新浪财经的股票数据，就没有办法通过常规的Requests库进行爬取，相应的处理方法
